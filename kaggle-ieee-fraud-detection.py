# -*- coding: utf-8 -*-
"""CSE519 HW2 Template.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OhCe5x6d2kS2d6QYZFMygIynXjAJ7jEw

# Homework 2 - IEEE Fraud Detection
"""



"""## Part 0 - Setup that is required to run the code (Required to load data)"""

from google.colab import drive
drive.mount('/content/gdrive')

cd '/content/gdrive/My Drive/Semester I/Data Science Fundamentals/Submission'

import pandas as pd
TRAIN_TRANSACTION_DATA = pd.read_csv('train_transaction.csv')
TRAIN_IDENTITY_DATA = pd.read_csv('train_identity.csv')
Skiena_Columns= ['TransactionID','TransactionDT','TransactionAmt','ProductCD','card4','card6','P_emaildomain','R_emaildomain','addr1','addr2','dist1','dist2','DeviceType','DeviceInfo','isFraud']

#JOIN with IDENTITY COLUMN

TRAIN_TRANSACTION_DATA_FINAL = pd.merge(pd.DataFrame(TRAIN_TRANSACTION_DATA),pd.DataFrame(TRAIN_IDENTITY_DATA), how='left', on = 'TransactionID')

#EXTRACTING COLUMNS IN THE ASSIGMENT

TRAIN_TRANSACTION_DATA_SKIENA=TRAIN_TRANSACTION_DATA_FINAL[Skiena_Columns]



"""For all parts below, answer all parts as shown in the Google document for Homework 2. Be sure to include both code that justifies your answer as well as text to answer the questions. We also ask that code be commented to make it easier to follow.

## Part 1 - Fraudulent vs Non-Fraudulent Transaction

## Part 1(a) - Preprocessing
"""

TRAIN_TRANSACTION_DATA_SKIENA.head(5)

pd.Series(TRAIN_TRANSACTION_DATA_SKIENA['addr1'].unique()).sort_values().head(5)

pd.Series(TRAIN_TRANSACTION_DATA_SKIENA['addr2'].unique()).sort_values().head(5)

pd.Series(TRAIN_TRANSACTION_DATA_SKIENA['dist1'].unique()).sort_values().head(5)

pd.Series(TRAIN_TRANSACTION_DATA_SKIENA['dist2'].unique()).sort_values().head(5)

from sklearn import preprocessing
import numpy as np

# PREPROCESSING 
#P_emaildomain, R_emaildomain and DeviceInfo with LabelEncoder since they contain String fields that can be labelled.

# Moreover, fields like emaildomain can go huge and hence OneHot Encoding is not a good idea

le_p = preprocessing.LabelEncoder()
le_p.fit(np.unique(TRAIN_TRANSACTION_DATA_SKIENA['P_emaildomain'].astype(str)))

le_r = preprocessing.LabelEncoder()
le_r.fit(np.unique(TRAIN_TRANSACTION_DATA_SKIENA['R_emaildomain'].astype(str)))

le_d = preprocessing.LabelEncoder()
le_d.fit(np.unique(TRAIN_TRANSACTION_DATA_SKIENA['DeviceInfo'].astype(str)))

TRAIN_TRANSACTION_DATA_SKIENA['R_emaildomain']=pd.DataFrame(TRAIN_TRANSACTION_DATA_SKIENA['R_emaildomain'].astype(str)).apply(le_r.transform)
TRAIN_TRANSACTION_DATA_SKIENA['P_emaildomain']=pd.DataFrame(TRAIN_TRANSACTION_DATA_SKIENA['P_emaildomain'].astype(str)).apply(le_p.transform)
TRAIN_TRANSACTION_DATA_SKIENA['DeviceInfo']=pd.DataFrame(TRAIN_TRANSACTION_DATA_SKIENA['DeviceInfo'].astype(str)).apply(le_d.transform)

##Shown above, addr1 and addr2 does not contain 0 and hence 0 can be put for nan values
##Shown above, dist1 and dist2 does contain 0 and hence -1 should be put for nan values
TRAIN_TRANSACTION_DATA_SKIENA['dist2'].fillna(-1.0,inplace=True)
TRAIN_TRANSACTION_DATA_SKIENA['dist1'].fillna(-1.0,inplace=True)
TRAIN_TRANSACTION_DATA_SKIENA['addr2'].fillna(0.0,inplace=True)
TRAIN_TRANSACTION_DATA_SKIENA['addr1'].fillna(0.0,inplace=True)



## TransactionDT looks like a timestamp in seconds and hence it is wise to extract data it contains in hours and use that


Onehot_hour=pd.DataFrame(pd.to_datetime(TRAIN_TRANSACTION_DATA_SKIENA['TransactionDT'],unit='s').dt.hour)
Onehot_hour.columns=['hour']
TRAIN_TRANSACTION_DATA_SKIENA['hour']=pd.DataFrame(Onehot_hour)


## ProductCD, card4 and card6 and DeviceType contains limited set values and can strongly influence decision making which will be shown further
## Hence separate features for them using OneHot Encoding
Onehot=pd.DataFrame(pd.get_dummies(TRAIN_TRANSACTION_DATA_SKIENA['ProductCD']))
TRAIN_TRANSACTION_DATA_SKIENA=TRAIN_TRANSACTION_DATA_SKIENA.join(Onehot)
Onehot=pd.DataFrame(pd.get_dummies(TRAIN_TRANSACTION_DATA_SKIENA['card4']))
TRAIN_TRANSACTION_DATA_SKIENA=TRAIN_TRANSACTION_DATA_SKIENA.join(Onehot)
Onehot=pd.DataFrame(pd.get_dummies(TRAIN_TRANSACTION_DATA_SKIENA['card6']))
TRAIN_TRANSACTION_DATA_SKIENA=TRAIN_TRANSACTION_DATA_SKIENA.join(Onehot)
Onehot=pd.DataFrame(pd.get_dummies(TRAIN_TRANSACTION_DATA_SKIENA['DeviceType']))
TRAIN_TRANSACTION_DATA_SKIENA=TRAIN_TRANSACTION_DATA_SKIENA.join(Onehot)

TRAIN_TRANSACTION_DATA_SKIENA.drop(columns=['card4','card6','ProductCD','DeviceType','TransactionDT'],inplace=True)

"""## Data after preprocessing and cleaning"""

TRAIN_TRANSACTION_DATA_SKIENA.head(10)

"""#### TransactionID can be skipped as that is a simple identity field and it will not influence a fraudulent nature of transaction.

#### Device Type and Fradulent nature
"""

TRAIN_T_D_FRAUD = TRAIN_TRANSACTION_DATA_SKIENA['isFraud']>0.02
TRAIN_T_D_NON_FRAUD = TRAIN_TRANSACTION_DATA_SKIENA['isFraud']<=0.02
TRAIN_T_D_FRAUD_DATA = TRAIN_TRANSACTION_DATA_SKIENA[TRAIN_T_D_FRAUD]
TRAIN_T_D_NON_FRAUD_DATA = TRAIN_TRANSACTION_DATA_SKIENA[TRAIN_T_D_NON_FRAUD]
TRAIN_TRANSACTION_DATA_FRAUD_TID = TRAIN_T_D_FRAUD_DATA['TransactionID']
TRAIN_TRANSACTION_DATA_NONFRAUD_TID = TRAIN_T_D_NON_FRAUD_DATA['TransactionID']

FRAUD_DEVICE_TYPE_MOBILE = TRAIN_T_D_FRAUD_DATA.loc[TRAIN_T_D_FRAUD_DATA['mobile']==1]
FRAUD_DEVICE_TYPE_DESKTOP = TRAIN_T_D_FRAUD_DATA.loc[TRAIN_T_D_FRAUD_DATA['desktop']==1]
NON_FRAUD_DEVICE_TYPE_MOBILE = TRAIN_T_D_NON_FRAUD_DATA.loc[TRAIN_T_D_NON_FRAUD_DATA['mobile']==1]
NON_FRAUD_DEVICE_TYPE_DESKTOP = TRAIN_T_D_NON_FRAUD_DATA.loc[TRAIN_T_D_NON_FRAUD_DATA['desktop']==1]

FRAUD_DEVICE_TYPE_COUNT_MOBILE = pd.DataFrame(FRAUD_DEVICE_TYPE_MOBILE).shape[0] 
FRAUD_DEVICE_TYPE_COUNT_DESKTOP = FRAUD_DEVICE_TYPE_DESKTOP.shape[0]
FRAUD_DEVICE_TYPE_COUNT_TOTAL = TRAIN_T_D_FRAUD_DATA.shape[0]


NON_FRAUD_DEVICE_TYPE_COUNT_MOBILE = pd.DataFrame(NON_FRAUD_DEVICE_TYPE_MOBILE).shape[0] 
NON_FRAUD_DEVICE_TYPE_COUNT_DESKTOP = NON_FRAUD_DEVICE_TYPE_DESKTOP.shape[0]
NON_FRAUD_DEVICE_TYPE_COUNT_TOTAL = TRAIN_T_D_NON_FRAUD_DATA.shape[0]

MOB = { 'NON_FRAUD':(NON_FRAUD_DEVICE_TYPE_COUNT_MOBILE*100.0/NON_FRAUD_DEVICE_TYPE_COUNT_TOTAL),'FRAUD':(FRAUD_DEVICE_TYPE_COUNT_MOBILE*100.0/FRAUD_DEVICE_TYPE_COUNT_TOTAL)}  
DESK = { 'NON_FRAUD':(NON_FRAUD_DEVICE_TYPE_COUNT_DESKTOP*100.0/NON_FRAUD_DEVICE_TYPE_COUNT_TOTAL),'FRAUD':(FRAUD_DEVICE_TYPE_COUNT_DESKTOP*100.0/FRAUD_DEVICE_TYPE_COUNT_TOTAL)}  
Q1_DEVICETYPE = pd.DataFrame({'Mobile': MOB,'Desktop': DESK})

Q1_DEVICETYPE.plot.bar()

"""### The above graph shows that in fraudulent transactions, Mobile and Desktop contributed equally whereas in non fraduent cases Mobile had a fairly small share which goes into proving that Mobile was used for a lot of fraudulent activities(about 28%) as compared to that of non fradulent (about 8%) and can possibly be used as a feature.

### TransactionDT can be converted to hour and used to analyse fradulent behaviour
"""

DT_FRAUD=pd.DataFrame(TRAIN_T_D_FRAUD_DATA.groupby('hour').size()/TRAIN_T_D_FRAUD_DATA.groupby('hour').size().sum())
DT_NON_FRAUD=pd.DataFrame(TRAIN_T_D_NON_FRAUD_DATA.groupby('hour').size()/TRAIN_T_D_NON_FRAUD_DATA.groupby('hour').size().sum())
DT_FRAUD.columns=['Fraud']
DT_FRAUD['Non Fraud'] = DT_NON_FRAUD
DT_FRAUD.plot.line()

"""### Figure shows that TransactionDT or 'hour of transaction' followed the same pattern. Therefore, there was nothing that the hour of transaction could tell."""

AMT_FRAUD=pd.DataFrame(TRAIN_T_D_FRAUD_DATA['TransactionAmt'])
AMT_NON_FRAUD=pd.DataFrame(TRAIN_T_D_NON_FRAUD_DATA['TransactionAmt'])

import pylab as pl
np.log(AMT_NON_FRAUD).plot.hist(bins=70)
pl.suptitle("Log of TransactionAmt for Non Fraud Transactions")

np.log(AMT_FRAUD).plot.hist(bins=70)
pl.suptitle("Log TransactionAmt for Fraud Transactions")

"""### Transaction Amt also shows a similar trend and hence is not a suitable candidate for feature.
### Note: Log is taken because transaction values are too large
"""

FRAUD_PRODUCT_CD = pd.DataFrame((TRAIN_T_D_FRAUD_DATA[['W','C','H','R','S']].sum() * 100/TRAIN_T_D_FRAUD_DATA[['W','C','H','R','S']].sum().sum()))
FRAUD_PRODUCT_CD.columns = ['Fraud']
FRAUD_PRODUCT_CD['Fraud'] = pd.DataFrame(FRAUD_PRODUCT_CD)
NON_FRAUD_PRODUCT_CD = pd.DataFrame(TRAIN_T_D_NON_FRAUD_DATA[['W','C','H','R','S']].sum() * 100 /TRAIN_T_D_NON_FRAUD_DATA[['W','C','H','R','S']].sum().sum())
FRAUD_PRODUCT_CD['Non Fraud'] = pd.DataFrame(NON_FRAUD_PRODUCT_CD)
FRAUD_PRODUCT_CD.transpose().plot.bar()

"""### NonFraud Transactions are dominated by W product type. 

### In case of fraudulent transactions, there is an unexpected rise in the percent of ProductType C. This can be used as feature and a purchase of Product Type C has a much higher chance of it being C when compare to other product types.
"""

NON_FRAUD_CARD_4= TRAIN_T_D_NON_FRAUD_DATA[['american express', 'discover', 'mastercard', 'visa']][(TRAIN_T_D_NON_FRAUD_DATA['american express']==1) |
                                                                                 (TRAIN_T_D_NON_FRAUD_DATA['discover']==1) |
                                                                                 (TRAIN_T_D_NON_FRAUD_DATA['mastercard']==1)|
                                                                                 (TRAIN_T_D_NON_FRAUD_DATA['visa']==1)
                                                                                ].sum()*100/TRAIN_T_D_NON_FRAUD_DATA.shape[0]
NON_FRAUD_CARD_6= TRAIN_T_D_NON_FRAUD_DATA[['charge card', 'credit', 'debit', 'debit or credit']][(TRAIN_T_D_NON_FRAUD_DATA['charge card']==1) |
                                                                                 (TRAIN_T_D_NON_FRAUD_DATA['credit']==1) |
                                                                                 (TRAIN_T_D_NON_FRAUD_DATA['debit']==1)|
                                                                                 (TRAIN_T_D_NON_FRAUD_DATA['debit or credit']==1)
                                                                                ].sum()*100/TRAIN_T_D_NON_FRAUD_DATA.shape[0]
FRAUD_CARD_4= TRAIN_T_D_FRAUD_DATA[['american express', 'discover', 'mastercard', 'visa']][
                                                                                 (TRAIN_T_D_FRAUD_DATA['american express']==1) |
                                                                                 (TRAIN_T_D_FRAUD_DATA['discover']==1) |
                                                                                 (TRAIN_T_D_FRAUD_DATA['mastercard']==1)|
                                                                                 (TRAIN_T_D_FRAUD_DATA['visa']==1)
                                                                                ].sum()*100/TRAIN_T_D_FRAUD_DATA.shape[0]
FRAUD_CARD_6= TRAIN_T_D_FRAUD_DATA[['charge card', 'credit', 'debit', 'debit or credit']][
                                                                                 (TRAIN_T_D_FRAUD_DATA['charge card']==1) |
                                                                                 (TRAIN_T_D_FRAUD_DATA['credit']==1) |
                                                                                 (TRAIN_T_D_FRAUD_DATA['debit']==1)|
                                                                                 (TRAIN_T_D_FRAUD_DATA['debit or credit']==1)
                                                                                ].sum()*100/TRAIN_T_D_FRAUD_DATA.shape[0]
NON_FRAUD_CARD_6 = pd.DataFrame(NON_FRAUD_CARD_6)
FRAUD_CARD_6 = pd.DataFrame(FRAUD_CARD_6)
FRAUD_CARD_6.columns = ['Fraud']
FRAUD_CARD_6['Non Fraud']  = NON_FRAUD_CARD_6
FRAUD_CARD_6.transpose().plot.bar()


NON_FRAUD_CARD_4 = pd.DataFrame(NON_FRAUD_CARD_4)
FRAUD_CARD_4 = pd.DataFrame(FRAUD_CARD_4)
FRAUD_CARD_4.columns = ['Fraud']
FRAUD_CARD_4['Non Fraud']  = NON_FRAUD_CARD_4
FRAUD_CARD_4.transpose().plot.bar()

"""### The card4 column shows cardtype. "credit" card shown an unexpected rise in percent in fraudulent cases and hence can be used as feature. "debit" on the other hand, shows a decrease in percent in case of fraudulent cases making a "debit" transaction much safer that "credit"

### Company issuing the card in card6 columns reveals no fruitful info,
"""

np.log(TRAIN_T_D_NON_FRAUD_DATA[TRAIN_T_D_NON_FRAUD_DATA['addr1']!=0.0].groupby('addr1').size().sort_values()).tail(100).plot.bar(figsize=(25,4))

A=pd.DataFrame(TRAIN_T_D_FRAUD_DATA[TRAIN_T_D_FRAUD_DATA['addr1']>1.0].groupby('addr1').size())
B=pd.DataFrame(TRAIN_T_D_NON_FRAUD_DATA[TRAIN_T_D_NON_FRAUD_DATA['addr1']!=0.0].groupby('addr1').size())

A.columns=['Fraud']
A['Non Fraud']= pd.DataFrame(B)
A['percent']= A['Fraud']*100/(A['Fraud'] + A['Non Fraud'])
A['percent'].sort_values().plot.bar(figsize=(20,4))
A.sort_values(by='percent').tail(7)

"""### In areas with addr1 as 391, 471, 501, 466, 483, 395 there is more than 45% fraudulent traffic"""

np.log(TRAIN_T_D_NON_FRAUD_DATA[TRAIN_T_D_NON_FRAUD_DATA['addr2']!=0.0].groupby('addr2').size()).tail(100).plot.bar(figsize=(25,4))

A=pd.DataFrame(TRAIN_T_D_FRAUD_DATA[TRAIN_T_D_FRAUD_DATA['addr2']!=0.0].groupby('addr2').size())
B=pd.DataFrame(TRAIN_T_D_NON_FRAUD_DATA[TRAIN_T_D_NON_FRAUD_DATA['addr2']!=0.0].groupby('addr2').size())

A.columns=['Fraud']
A['Non Fraud']= pd.DataFrame(B)

A['percent']= A['Fraud']*100/(A['Fraud'] + A['Non Fraud'])
A['percent'].sort_values().plot.bar(figsize=(20,4))
A.sort_values(by='percent').tail(7)

"""### 66% transactions with addr2 are fraudulent"""

A=pd.DataFrame(TRAIN_T_D_FRAUD_DATA[TRAIN_T_D_FRAUD_DATA['dist1']!=-1.0].groupby('dist1').size())
B=pd.DataFrame(TRAIN_T_D_NON_FRAUD_DATA[TRAIN_T_D_NON_FRAUD_DATA['dist1']!=-1.0].groupby('dist1').size())
A.columns=['Fraud']

A['Non Fraud']= pd.DataFrame(B)

A['percent']= A['Fraud']*100/(A['Fraud'] + A['Non Fraud'])
A['percent'].sort_values().plot.bar(figsize=(200,4))
A.sort_values(by='percent').tail(7)



"""### 70+ % transactions through with dist 1 as '2148' are fraudulent"""

TRAIN_T_D_FRAUD_DATA[TRAIN_T_D_FRAUD_DATA['dist2']!=-1.0].groupby('dist2').size().sort_values().tail(100).plot.bar(figsize=(25,4))

A=pd.DataFrame(TRAIN_T_D_NON_FRAUD_DATA[TRAIN_T_D_NON_FRAUD_DATA['dist2']!=-1.0].groupby('dist2').size())
B=pd.DataFrame(TRAIN_T_D_FRAUD_DATA[TRAIN_T_D_FRAUD_DATA['dist2']!=-1.0].groupby('dist2').size())
A.columns=['Non Fraud']
A['Fraud']= pd.DataFrame(B)

A['percent']= A['Fraud']*100/(A['Fraud'] + A['Non Fraud'])

TRAIN_T_D_FRAUD_DATA.groupby('dist2').size().head(200).tail(25)

A.dropna(subset=['percent'], inplace=True)
A['percent'].sort_values().plot.bar(figsize=(50,4))

A.sort_values(by='percent').tail(5)

"""## 95% of the transactions reported for 382 dist2 are fraud"""



"""## Part 2 - Transaction Frequency"""

TRAIN_TRANSACTION_TRANSACTIONDT_ADDR2 = TRAIN_TRANSACTION_DATA_SKIENA[['hour', 'addr2']]
COUNT_OF_ADDR2=pd.DataFrame(TRAIN_TRANSACTION_TRANSACTIONDT_ADDR2.groupby('addr2').size())
COUNT_OF_ADDR2.columns=['Count']
COUNT_OF_ADDR2.sort_values(by='Count').tail(1)
TRAIN_TRANSACTION_TRANSACTIONDT_ADDR2_MAX_FREQ_EIGHTY_SEVEN_HOUR_CRITERIA = TRAIN_TRANSACTION_TRANSACTIONDT_ADDR2.loc[TRAIN_TRANSACTION_TRANSACTIONDT_ADDR2['addr2']==87.0]
TRAIN_TRANSACTION_TRANSACTIONDT_ADDR2_MAX_FREQ_EIGHTY_SEVEN_HOUR_CRITERIA.groupby('hour').sum().plot.bar(figsize=(10,4))

"""* ### The highest data is for addr2 which is most probably a country code. This data was discussed and heavily supported in kaggle's discussion threads. https://www.kaggle.com/c/ieee-fraud-detection/discussion/102910#latest-595293

* ### The article discusses the waking time of an average american  https://whygetupearly.com/whats-the-average-bedtime-for-adults/

* ### The average american wakes up at 6:30am and sleeps at 11:30pm. Roughly estimating these two to be from 5 to 12, the waking hours relative to this data is 0 to 5 and 12 to 23.

## Part 3 - Product Code
"""

W_COST = TRAIN_TRANSACTION_DATA_SKIENA.loc[TRAIN_TRANSACTION_DATA_SKIENA['W']==1]
C_COST = TRAIN_TRANSACTION_DATA_SKIENA.loc[TRAIN_TRANSACTION_DATA_SKIENA['C']==1]
H_COST = TRAIN_TRANSACTION_DATA_SKIENA.loc[TRAIN_TRANSACTION_DATA_SKIENA['H']==1]
R_COST = TRAIN_TRANSACTION_DATA_SKIENA.loc[TRAIN_TRANSACTION_DATA_SKIENA['R']==1]
S_COST = TRAIN_TRANSACTION_DATA_SKIENA.loc[TRAIN_TRANSACTION_DATA_SKIENA['S']==1]
MeanH=H_COST['TransactionAmt'].sum()/H_COST.shape[0]
MeanW=W_COST['TransactionAmt'].sum()/W_COST.shape[0]
MeanC=C_COST['TransactionAmt'].sum()/C_COST.shape[0]
MeanR=R_COST['TransactionAmt'].sum()/R_COST.shape[0]
MeanS=S_COST['TransactionAmt'].sum()/S_COST.shape[0]
cost = {'H':MeanH,'W':MeanW,'C':MeanC,'R':MeanR,'S':MeanS}
COST = pd.DataFrame({'Average Price': cost})
COST.plot.bar()

"""* ### Product type R seems expensive at around 168 avg cost per unit.
* ### Product type C seems cheapest at around 40 avg cost per unit.
* ### The above graph shows average price of each product type.
* ### It is calculated using mean. (Total Transaction AMT in ProductCD)/(All respective ProductCD transactions)

## Part 4 - Correlation Coefficient
"""

TOD_AND_AMT = TRAIN_TRANSACTION_DATA_SKIENA[['TransactionAmt','hour']].groupby('hour').size()
TOD_AND_AMT=pd.DataFrame(TOD_AND_AMT)
TOD_AND_AMT.columns=['Amt']
TOD_AND_AMT['hour']=pd.DataFrame(TRAIN_TRANSACTION_DATA_SKIENA['hour'].unique())
TOD=pd.DataFrame()
TOD['Amt']=TOD_AND_AMT['Amt']
TOD['Time']=TOD_AND_AMT['hour']
TOD=pd.DataFrame()
TOD=pd.DataFrame(np.array(TRAIN_TRANSACTION_DATA_SKIENA[['TransactionAmt','hour']].groupby('hour').size()))
pd.DataFrame(np.array(TRAIN_TRANSACTION_DATA_SKIENA[['TransactionAmt','hour']].groupby('hour').size()))
TOD.columns = ['TransactionAmt']
TOD.plot.bar()

TRAIN_TRANSACTION_DATA_SKIENA.corr()

"""* ### Transaction Amt shows similar pattern to the activity and hence complements each other.

* ### When people are awake they make more purchases.

* ### The correlation below transactionamt and hour is 0.04

## Part 5 - Interesting Plot
"""

W_ANALYSIS=TRAIN_TRANSACTION_DATA_SKIENA.loc[TRAIN_TRANSACTION_DATA_SKIENA['W']==1]
NON_W_ANALYSIS=TRAIN_TRANSACTION_DATA_SKIENA.loc[TRAIN_TRANSACTION_DATA_SKIENA['W']!=1]
W_ANALYSIS.loc[W_ANALYSIS['dist2']!=-1].sum().sum()

NON_W_ANALYSIS.loc[NON_W_ANALYSIS['dist1']!=-1].sum().sum()

"""* #### The above analysis shows that there is no data in dist2 for Product code : 'W'

* #### The above analysis also shows that there is no data in dist1 for Product code : 'H','C','R','S'

* #### There is a school of thought (https://www.kaggle.com/c/ieee-fraud-detection/discussion/107791) that puts 'ProductCD' as a type of transaction.

* ### If that were true, and give the high percent for ProductCD 'W', it is probably the mode of online payment using 'Web' and hence the high number.
"""

plt.subplots(figsize=(20,10))
FRAUD_P=pd.DataFrame(TRAIN_T_D_FRAUD_DATA.groupby('P_emaildomain').size())

plt.subplots(figsize=(20,10))
NON_FRAUD_P=pd.DataFrame(TRAIN_T_D_NON_FRAUD_DATA.groupby('P_emaildomain').size())

FRAUD_P['NON FRAUD'] = pd.DataFrame(TRAIN_T_D_NON_FRAUD_DATA.groupby('P_emaildomain').size())

np.log(FRAUD_P).plot.bar(figsize=(20,10))

FRAUD_P['ratio']=FRAUD_P['FRAUD']/FRAUD_P['NON FRAUD']

FRAUD_P['percent']=FRAUD_P['FRAUD']* 100/(FRAUD_P['NON FRAUD']+FRAUD_P['FRAUD'])

FRAUD_P['percent'].plot.bar(figsize=(20,10))

le_p.inverse_transform([39,29,0,37])

TRAIN_TRANSACTION_DATA_SKIENA[TRAIN_TRANSACTION_DATA_SKIENA['P_emaildomain']==39].groupby('isFraud').size()

"""* ### Another insight that one can find by analysing the P_emaildomain or Purchaser's email domain is that there is one email domain which stands out "protonmail"

* ### 40% transactions made by protonmail are fraudulent.

* ### There are 31 fraudulent transaction of protonmail and 45 non fraudulent. 

* ### Following closely are 'mail.com', 'aim.com' and 'outlook.es'

## Part 6 - Prediction Model
"""

TRAIN = TRAIN_TRANSACTION_DATA_SKIENA.copy(deep=False)
TRAIN['R_emaildomain'] = le_r.inverse_transform(TRAIN['R_emaildomain'])
TRAIN['P_emaildomain'] = le_p.inverse_transform(TRAIN['P_emaildomain'])
TRAIN['R_emaildomain']='R_'+TRAIN['R_emaildomain'].astype(str)
TRAIN['P_emaildomain']='P_'+TRAIN['P_emaildomain'].astype(str)
Onehot=pd.DataFrame(pd.get_dummies(TRAIN['R_emaildomain']))
TRAIN=TRAIN.join(Onehot)
Onehot=pd.DataFrame(pd.get_dummies(TRAIN['P_emaildomain']))
TRAIN=TRAIN.join(Onehot)




TRAIN.drop(columns=['R_emaildomain','P_emaildomain'], inplace=True)



### Load data

import pandas as pd
TEST_TRANSACTION_DATA = pd.read_csv('test_transaction.csv')
TEST_IDENTITY_DATA = pd.read_csv('test_identity.csv')



TEST_TRANSACTION_DATA_FINAL = pd.merge(pd.DataFrame(TEST_TRANSACTION_DATA),pd.DataFrame(TEST_IDENTITY_DATA), how='left', on = 'TransactionID')

#Extracting only for skiena's columns
Skiena_Columns= ['TransactionID','TransactionDT','TransactionAmt','ProductCD','card4','card6','P_emaildomain','R_emaildomain','addr1','addr2','dist1','dist2','DeviceType','DeviceInfo']
TEST_TRANSACTION_DATA_SKIENA=TEST_TRANSACTION_DATA_FINAL[Skiena_Columns]

##Drop DeviceInfo

TEST_TRANSACTION_DATA_SKIENA.drop(columns=['DeviceInfo'], inplace=True)




#Cleaning 
TEST_TRANSACTION_DATA_SKIENA['R_emaildomain']='R_'+TEST_TRANSACTION_DATA_SKIENA['R_emaildomain'].astype(str)
TEST_TRANSACTION_DATA_SKIENA['P_emaildomain']='P_'+TEST_TRANSACTION_DATA_SKIENA['P_emaildomain'].astype(str)
TEST_TRANSACTION_DATA_SKIENA['dist2'].fillna(-1.0,inplace=True)
TEST_TRANSACTION_DATA_SKIENA['dist1'].fillna(-1.0,inplace=True)
TEST_TRANSACTION_DATA_SKIENA['addr2'].fillna(0.0,inplace=True)
TEST_TRANSACTION_DATA_SKIENA['addr1'].fillna(0.0,inplace=True)

#Onehot encoding

Onehot=pd.DataFrame(pd.get_dummies(TEST_TRANSACTION_DATA_SKIENA['R_emaildomain']))
TEST_TRANSACTION_DATA_SKIENA=TEST_TRANSACTION_DATA_SKIENA.join(Onehot)
Onehot=pd.DataFrame(pd.get_dummies(TEST_TRANSACTION_DATA_SKIENA['P_emaildomain']))
TEST_TRANSACTION_DATA_SKIENA=TEST_TRANSACTION_DATA_SKIENA.join(Onehot)
Onehot=pd.DataFrame(pd.get_dummies(TEST_TRANSACTION_DATA_SKIENA['ProductCD']))
TEST_TRANSACTION_DATA_SKIENA=TEST_TRANSACTION_DATA_SKIENA.join(Onehot)
Onehot=pd.DataFrame(pd.get_dummies(TEST_TRANSACTION_DATA_SKIENA['card4']))
TEST_TRANSACTION_DATA_SKIENA=TEST_TRANSACTION_DATA_SKIENA.join(Onehot)
Onehot=pd.DataFrame(pd.get_dummies(TEST_TRANSACTION_DATA_SKIENA['card6']))
TEST_TRANSACTION_DATA_SKIENA=TEST_TRANSACTION_DATA_SKIENA.join(Onehot)
Onehot=pd.DataFrame(pd.get_dummies(TEST_TRANSACTION_DATA_SKIENA['DeviceType']))
TEST_TRANSACTION_DATA_SKIENA=TEST_TRANSACTION_DATA_SKIENA.join(Onehot)


#Drop one hot encoded columns

TEST_TRANSACTION_DATA_SKIENA.drop(columns=['card4','card6','ProductCD','R_emaildomain','P_emaildomain','DeviceType'],inplace=True)

#Cleaning transactiondt

Onehot_hour=pd.DataFrame(pd.to_datetime(TEST_TRANSACTION_DATA_SKIENA['TransactionDT'],unit='s').dt.hour)

Onehot_hour.columns=['hour']

TEST_TRANSACTION_DATA_SKIENA['hour']=pd.DataFrame(Onehot_hour)

TEST_TRANSACTION_DATA_SKIENA.drop(columns=['TransactionDT'],inplace=True)

#### TEST DATA SETUP END ##################

import matplotlib.pyplot as plt
plt.subplots(figsize=(50,15))
import seaborn as sns
sns.heatmap(np.abs(TRAIN.corr()))

"""### The above heatmap shows that all columns are not required. Hence, only few columns with correlation greater  > 0.4 absolute value


### Two models are trained below to predict. Logistic and Linear Regression
"""

from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.model_selection import train_test_split

from sklearn import metrics

COLS=['mobile','desktop','credit','debit','W','R','H','C',
      'P_yahoo.com','P_hotmail.com','P_gmail.com'
      ,'R_yahoo.com','R_hotmail.com','R_gmail.com',
      'addr1','addr2','dist1','dist2']
X=TRAIN[COLS]
Y=TRAIN_TRANSACTION_DATA_SKIENA['isFraud']

X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(X, Y, test_size=0.3)

clf = LogisticRegression(solver='lbfgs')
clf.fit(X_TRAIN, Y_TRAIN)

PREDS = clf.predict(X_TEST)

print(metrics.classification_report(Y_TEST, PREDS))

"""Write your answer here"""

reg = LinearRegression().fit(X_TRAIN, Y_TRAIN)
import numpy as np;
y_pred = reg.predict(X_TEST)

mae = metrics.mean_absolute_error(Y_TEST, y_pred)
msq = metrics.mean_squared_error(Y_TEST, y_pred)
rmse = np.sqrt(metrics.mean_squared_error(Y_TEST, y_pred))

print('Mean Absolute Error:'+str(mae))
print('Mean Squared Error:'+str(msq))
print('Root Mean Squared Error:'+str(rmse))

SAMPLE_SUBMISSION = pd.read_csv('sample_submission.csv')
TRANSACTIONIDS_JOIN = pd.merge(pd.DataFrame(SAMPLE_SUBMISSION),pd.DataFrame(TEST_TRANSACTION_DATA_SKIENA), how='left', on = 'TransactionID')
TRANSACTIONIDS_JOIN=TRANSACTIONIDS_JOIN[COLS]

FRAUD_COLUMN_LOG=clf.predict(TRANSACTIONIDS_JOIN)
FRAUD_COLUMN_LIN= reg.predict(TRANSACTIONIDS_JOIN)
KAGGLE_UPLOAD_LOG=pd.DataFrame()
KAGGLE_UPLOAD_LIN=pd.DataFrame()
KAGGLE_UPLOAD_LOG['TransactionID']=SAMPLE_SUBMISSION['TransactionID']
KAGGLE_UPLOAD_LIN['TransactionID']=SAMPLE_SUBMISSION['TransactionID']
FRAUD_COLUMN_LOG= pd.DataFrame(FRAUD_COLUMN_LOG)
FRAUD_COLUMN_LIN= pd.DataFrame(FRAUD_COLUMN_LIN)
FRAUD_COLUMN_LOG.columns=['isFraud']
FRAUD_COLUMN_LIN.columns=['isFraud']
KAGGLE_UPLOAD_LOG['isFraud']=pd.DataFrame(FRAUD_COLUMN_LOG['isFraud'])
KAGGLE_UPLOAD_LIN['isFraud']=pd.DataFrame(FRAUD_COLUMN_LIN['isFraud'])

KAGGLE_UPLOAD_LIN.to_csv("LinearRegression.csv")
KAGGLE_UPLOAD_LIN.to_csv("LogisticRegression.csv")

"""## Part 7 - Final Result

Report the rank, score, number of entries, for your highest rank. Include a snapshot of your best score on the leaderboard as confirmation. Be sure to provide a link to your Kaggle profile. Make sure to include a screenshot of your ranking. Make sure your profile includes your face and affiliation with SBU.

Kaggle Link: https://www.kaggle.com/abdullahmitkar

Highest Rank: 5343

Score: 0.8057

Number of entries: 4

INCLUDE IMAGE OF YOUR KAGGLE RANKING
"""

import cv2
plt.subplots(figsize=(50,15))
img=cv2.imread('Capture_Kaggle.PNG')
plt.imshow(img)

